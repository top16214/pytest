{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "headers = {\n",
    "    'user-agent':\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36'\n",
    "}\n",
    "url = r'https://zhuanlan.zhihu.com/python-programming'\n",
    "r = requests.get(url, headers=headers)\n",
    "# print(r.content)\n",
    "print(r.status_code)  # 提取请求状态码\n",
    "print(r.url)  # 当前请求的url\n",
    "# r.text # 网页源代码字符型\n",
    "# r.content # 网页源代码bytes型\n",
    "# # 两种编码\n",
    "print(\"encoding:\" + r.encoding + \"\\n\"\n",
    "      \"apparent_encoding:\" + r.apparent_encoding)\n",
    "# # 另一个需要提起的是r.request，如果用r.headers则返回的是response的headers，如果我们想获取发送请求时带的headers，就要r.requesheaders则\n",
    "# print(r.headers)\n",
    "print(r.request.headers)\n",
    "# r.history\n",
    "\"\"\"在抓取网页的时候，可能会出现这样的情况：r.text的结果是一些看不懂的字符，即默认使用的字符集r.encoding是错误的，这样我们就无法提取到正确的信息，此时我们就需要使用r.content使用正确的字符集r.apparent_encoding转化成我们真正想要的字符。可以看下面这个例子\"\"\"\n",
    "# 常用下面两种方法的其中一种\n",
    "print(r.content.decode(r.apparent_encoding))\n",
    "print(r.text.encode(r.encoding).decode(r.apparent_encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "headers = {\n",
    "    'user-agent':\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36'\n",
    "}\n",
    "url = r\"https://fulibus.net/\"\n",
    "r = requests.get(url, headers=headers)\n",
    "# r.status_code\n",
    "# print(r.text)\n",
    "bsObj = BeautifulSoup(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import urllib.request\n",
    "\n",
    "source = urllib.request.urlopen(\n",
    "    'https://pythonprogramming.net/parsememcparseface/')\n",
    "soup = bs.BeautifulSoup(source, 'lxml')\n",
    "\n",
    "print(source.code)\n",
    "# nav = soup.nav\n",
    "# for url in nav.find_all('a'):\n",
    "#     print(url.get('href'))\n",
    "\n",
    "# for div in soup.find_all('div', class_='body'):\n",
    "#     print(div.text)\n",
    "\n",
    "# title of the page\n",
    "# print(soup.title)\n",
    "# get attributes:\n",
    "# print(soup.title.name)\n",
    "# get values:\n",
    "# print(soup.title.string)\n",
    "\n",
    "# beginning navigation:\n",
    "# print(soup.title.parent.name)\n",
    "\n",
    "# getting specific values:\n",
    "# print(soup.p)\n",
    "\n",
    "# print(soup.find_all('p'))\n",
    "\n",
    "# print(soup.get_text())\n",
    "\n",
    "# for paragraph in soup.find_all('p'):\n",
    "#     print(paragraph.string)\n",
    "#     print(str(paragraph.text))\n",
    "\n",
    "# for url in soup.find_all('a'):\n",
    "#     print(url.get('href'))\n",
    "\n",
    "# type(soup.find_all(\"a\"))\n",
    "\n",
    "# print(soup.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# url = r\"https://osdn.net/dl/linuxlite/linux-lite-5.0-64bit.iso\"\n",
    "# url = r\"https://video.zhihu.com/video/1249468127664373760?\"\n",
    "# url = r\"https://news.163.com/20/0601/16/FE25U31O000189FH.html\"\n",
    "url = r\"https://www.hjenglish.com/yufadaquan/p1221891/\"\n",
    "\n",
    "with urllib.request.urlopen(url) as response:\n",
    "    html = response.read().decode()\n",
    "    #     print(response.geturl())\n",
    "    #     print(html)\n",
    "    bs = BeautifulSoup(html, \"lxml\")\n",
    "    #     print(bs.title.text)\n",
    "\n",
    "    staff = bs.find('div', class_='article-content')\n",
    "    #     help(staff)\n",
    "    for element in staff.find_all('<p>'):\n",
    "        print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "from urllib import parse\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "r = request.urlopen(r\"https://www.python.org/\")\n",
    "print(\"status_code:%s\\nReal URL:%s\\nLength:%d bytes\" %\n",
    "      (r.code, r.geturl(), r.length))\n",
    "data = r.read()  # data is the raw data\n",
    "print(\"r.read()--->%s\" % type(data))\n",
    "print(\"data的长度是 %d 字节\" % len(data))\n",
    "html = data.decode()  # html is the human-readable data\n",
    "print(\"read the response again,return NOTHING:\\n%s\" % r.read())\n",
    "print(\"因为response内容一旦被read以后，链接被终止\\n\")\n",
    "\n",
    "# 处理含有参数的url\n",
    "# url = r\"https://www.youtube.com/watch?v=L-iYOmU63IE&list=PL-g0fdC5RMbpqZ0bmvJTgVTS4tS3txRVp&index=3\"\n",
    "params = {\n",
    "    \"v\": \"L-iYOmU63IE\",\n",
    "    \"list\": \"PL-g0fdC5RMbpqZ0bmvJTgVTS4tS3txRVp\",\n",
    "    \"index\": \"3\"\n",
    "}\n",
    "querystring = parse.urlencode(params)\n",
    "url = \"https://www.youtube.com/watch\" + \"?\" + querystring\n",
    "r = request.urlopen(url)\n",
    "html = r.read().decode()\n",
    "print(html[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL Error\n",
    "\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "\n",
    "# trying to read the URL but with no internet connectivity\n",
    "try:\n",
    "    x = urllib.request.urlopen('https://www.go56ogle.com')\n",
    "    print(x.read())\n",
    "\n",
    "# Catching the exception generated\n",
    "except Exception as e:\n",
    "    print(str(e))\n",
    "\n",
    "# trying to read the URL\n",
    "try:\n",
    "    x = urllib.request.urlopen('https://www.google.com/search?q=test')\n",
    "    print(x.read())\n",
    "# Catching the exception generated\n",
    "except Exception as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open(r\"D:\\codePlayground\\pytest\\test.html\", \"r\",\n",
    "          encoding=\"UTF-8\") as fobj:\n",
    "    html = fobj.read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    element = soup.find('div', class_=\"single-content\")\n",
    "    #     print(bs.find('h1', class_=\"entry-title\").string)\n",
    "    print(len(element.p))\n",
    "    for k,v in enumerate(element.p):\n",
    "        print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-14T06:54:21.996436Z",
     "start_time": "2021-01-14T06:54:19.019866Z"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding:UTF-8 -*-\n",
    "# 尝试抓取《恶之教典》\n",
    "# date: 2020 / 12 / 16\n",
    "\n",
    "from urllib import request\n",
    "from urllib import parse\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "\n",
    "headers={}\n",
    "url = \"\"\n",
    "\n",
    "def set_para():\n",
    "    \"\"\"set the precedent parameters\"\"\"\n",
    "    global headers,url\n",
    "    headers = {\n",
    "        'user-agent':\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36'\n",
    "    }\n",
    "    url = r'https://www.cbxs.net/zhentan/8177/'\n",
    "\n",
    "def get_page_url(url,headers):\n",
    "    \"\"\"get novel's page URLs\"\"\"\n",
    "    req = request.Request(url, headers=headers)\n",
    "    response = request.urlopen(req)\n",
    "    # print(response.code)\n",
    "    # print(response.peek())\n",
    "    plain_html = response.read().decode()\n",
    "    # print(plain_html)\n",
    "    # print(response.geturl())\n",
    "    # print(response.length)\n",
    "    bsObj = BeautifulSoup(plain_html, \"lxml\")\n",
    "\n",
    "    staff = bsObj.find('div', class_=\"book-list clearfix\")\n",
    "\n",
    "    text_dict = dict()\n",
    "    for k,v in enumerate(staff.dl):\n",
    "        if k == 0:\n",
    "            continue\n",
    "        page_url = parse.urljoin(url,v.a['href'])\n",
    "    #     print(page_url,v.string)\n",
    "        text_dict[v.string] = page_url\n",
    "    # print(str(text_dict))\n",
    "    return text_dict\n",
    "\n",
    "    \n",
    "def save2txt(text_dic):\n",
    "    \"\"\"save contents to txt file\"\"\"\n",
    "    for i in text_dic:\n",
    "        with open(i+\".txt\",'w',encoding=\"utf-8\") as fd:\n",
    "            url = text_dic[i]\n",
    "            req = request.Request(url, headers=headers)\n",
    "            response = request.urlopen(req)\n",
    "            plain_html = response.read().decode()\n",
    "            bsObj = BeautifulSoup(plain_html, \"lxml\")\n",
    "            contents = bsObj.find('div', class_=\"nr1\")\n",
    "            # 这里打住，先显示测试内容再说\n",
    "    \n",
    "def main():\n",
    "    \"\"\"program entry\"\"\"\n",
    "    set_para()\n",
    "    text_dic = get_page_url(url,headers)\n",
    "    save2txt(text_dic)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:33:42.616858Z",
     "start_time": "2021-01-16T09:33:40.577551Z"
    }
   },
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "from urllib import parse\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "\n",
    "headers = {\n",
    "        'user-agent':\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36'\n",
    "    }\n",
    "url = r'https://www.cbxs.net/zhentan/8177/213151.html'\n",
    "\n",
    "req = request.Request(url, headers=headers)\n",
    "response = request.urlopen(req)\n",
    "plain_html = response.read().decode()\n",
    "bsObj = BeautifulSoup(plain_html, \"lxml\")\n",
    "staff = bsObj.find('div', id=\"nr1\")\n",
    "\n",
    "novel_contents = list()\n",
    "for child in staff.children:\n",
    "    if isinstance(child, NavigableString):\n",
    "        continue\n",
    "\n",
    "    if child.strong:\n",
    "        novel_contents.append(\"\\n\"+child.string+\"\\n\")\n",
    "        continue\n",
    "    novel_contents.append(child.string+\"\\n\")\n",
    "\n",
    "with open(\"myNovel.txt\",\"w\",encoding=\"utf-8\") as fd:\n",
    "    fd.write(\"  \".join(novel_contents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T07:16:31.826903Z",
     "start_time": "2021-01-20T07:16:29.388875Z"
    }
   },
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "from urllib import parse\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {\n",
    "        'user-agent':\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36'\n",
    "    }\n",
    "url = r'https://www.cbxs.net/zhentan/8177/213151.html'\n",
    "\n",
    "req = request.Request(url, headers=headers)\n",
    "response = request.urlopen(req)\n",
    "# print(response.code)\n",
    "# print(response.peek())\n",
    "plain_html = response.read().decode()\n",
    "# print(plain_html)\n",
    "# print(response.geturl())\n",
    "# print(response.length)\n",
    "bsObj = BeautifulSoup(plain_html, \"lxml\")\n",
    "staff = bsObj.find('div', id=\"nr1\")\n",
    "# print(staff)\n",
    "# print(staff.p.string)\n",
    "\n",
    "for child in staff.children:\n",
    "    print(\" \"*2+child.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "# 用來爬取段落中含有注釋的特殊網頁\n",
    "# 例如第一章 https://www.cbxs.net/zhentan/8177/213135.html\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup,NavigableString\n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "headers = {\n",
    "        'user-agent':\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36'\n",
    "    }\n",
    "url = r'https://www.cbxs.net/zhentan/8177/213135.html'\n",
    "\n",
    "r = requests.get(url, headers=headers)\n",
    "r.encoding = 'utf-8'\n",
    "\n",
    "bs = BeautifulSoup(r.text,'html.parser')\n",
    "\n",
    "cc = bs.find('div',id = \"nr1\")\n",
    "\n",
    "io_str = StringIO()\n",
    "for child in cc.children:\n",
    "    current_line = \"\"\n",
    "    if isinstance(child,NavigableString):\n",
    "        continue\n",
    "\n",
    "    # 含注釋内容的特殊行\n",
    "    if child.span:\n",
    "        previous_str = child.span.previous_sibling\n",
    "        next_str = child.span.next_sibling\n",
    "        current_line = \"\".join([\" \"*4, previous_str,\"*\",next_str])\n",
    "        current_line += \"\".join([\"\\n【注】\",child.span.get(\"data-note\"),\"\\n\"])\n",
    "    else:  \n",
    "        # 一般的行，不含注釋内容的行\n",
    "        current_line = \"\".join([\" \"*4,child.text,\"\\n\"])\n",
    "    \n",
    "    io_str.write(current_line)\n",
    "    \n",
    "\n",
    "# with open(\"charter1.txt\",\"wb\") as fd:\n",
    "#     contents = bytes(io_str.getvalue(),encoding=\"utf-8\")\n",
    "#     fd.write(contents)\n",
    "\n",
    "with open(\"charter1.txt\",\"w\",encoding=\"utf-8\") as fd:\n",
    "    fd.write(io_str.getvalue())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}